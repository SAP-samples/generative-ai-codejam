{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Multimodal Models\n",
    "\n",
    "Multimodal models can use different inputs such as text, audio and images. In Generative AI Hub on SAP AI Core you can access `gpt-4o` which is multimodal.\n",
    "\n",
    "ðŸ‘‰ If you have not deployed the gpt-4o model in exercise 01 then go back to [01-deploy-model](01-deploy-model.md) and this time deploy gpt-4o.\n",
    "\n",
    "ðŸ‘‰ Then assign the deployment id to the `LLM_DEPLOYMENT_ID` in [variables.py](variables.py).\n",
    "\n",
    "ðŸ‘‰ Now run the code snippet below to get a description for the AI Foundation Architecture. These descriptions can then for example be used as alternative text for screen readers or other assistive tech.\n",
    "\n",
    "ðŸ‘‰ Download your own image and play around with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import variables\n",
    "\n",
    "with open('/home/user/projects/generative-ai-codejam/.aicore-config.json', 'r') as config_file:\n",
    "    config_data = json.load(config_file)\n",
    "\n",
    "os.environ[\"AICORE_AUTH_URL\"]=config_data[\"url\"]+\"/oauth/token\"\n",
    "os.environ[\"AICORE_CLIENT_ID\"]=config_data[\"clientid\"]\n",
    "os.environ[\"AICORE_CLIENT_SECRET\"]=config_data[\"clientsecret\"]\n",
    "os.environ[\"AICORE_BASE_URL\"]=config_data[\"serviceurls\"][\"AI_API_URL\"]\n",
    "\n",
    "os.environ[\"AICORE_RESOURCE_GROUP\"]=variables.RESOURCE_GROUP\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents/ai-foundation-architecture.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "message= {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe the images as an alternative text\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "            }\n",
    "        ]}\n",
    "    \n",
    "\n",
    "model = ChatOpenAI(deployment_id=variables.LLM_DEPLOYMENT_ID)\n",
    "\n",
    "response = model.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents/bananabread.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "message= {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Extract the ingredients and instructions in two different json files\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "            }\n",
    "        ]}\n",
    "    \n",
    "\n",
    "model = ChatOpenAI(deployment_id=variables.LLM_DEPLOYMENT_ID)\n",
    "\n",
    "response = model.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Next exercise](09-orchestration-service.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
