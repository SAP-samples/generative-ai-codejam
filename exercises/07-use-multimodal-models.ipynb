{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Multimodal Models\n",
    "\n",
    "Multimodal models can use different inputs such as text, audio and images. In Generative AI Hub on SAP AI Core you can access `gpt-4o` which is multimodal.\n",
    "\n",
    "ðŸ‘‰ If you have not deployed one of the gpt-4o models in previous exercises, then go back to the model library and deploy a model that can also process images.\n",
    "\n",
    "ðŸ‘‰ Then assign the deployment id to the `LLM_DEPLOYMENT_ID` in [variables.py](variables.py).\n",
    "\n",
    "ðŸ‘‰ Now run the code snippet below to get a description for the AI Foundation Architecture. These descriptions can then for example be used as alternative text for screen readers or other assistive tech.\n",
    "\n",
    "ðŸ‘‰ Download your own image and play around with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init_env\n",
    "import variables\n",
    "\n",
    "init_env.set_environment_variables()\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents/ai-foundation-architecture.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "message= {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe the images as an alternative text\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "            }\n",
    "        ]}\n",
    "    \n",
    "\n",
    "model = ChatOpenAI(deployment_id=variables.LLM_DEPLOYMENT_ID)\n",
    "\n",
    "response = model.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try other images\n",
    "I for example love banana bread. Do you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents/bananabread.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "message= {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Extract the ingredients and instructions in two different json files\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "            }\n",
    "        ]}\n",
    "    \n",
    "\n",
    "model = ChatOpenAI(deployment_id=variables.LLM_DEPLOYMENT_ID)\n",
    "\n",
    "response = model.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Next exercise](08-deploy-orchestration-service.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
