{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the Retrieval for a Retrieval Augmented Generation (RAG) Use Case\n",
    "Now that we have all our context information stored in SAP HANA Cloud Vector Store, we can start asking the LLM questions about SAP AI Services. This time the model will not respond from it's knowledge base, that is what it knows from it's training data but the retriever will check for relevant context information in our vector database and send that text chunk to the LLM to read through before responding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain_community.vectorstores.hanavector import HanaDB\n",
    "from hdbcli import dbapi\n",
    "\n",
    "import configparser\n",
    "import variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Change the **deployment ids** according to your deployments from exercise [01-deploy-model](01-deploy-model.md).\n",
    "\n",
    "ğŸ‘‰ SET the **EMBEDDING_TABLE** to **\"EMBEDDINGS_SAP_AI_SERVICES_>add your name here<\"** like in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_DEPLOYMENT_ID = variables.EMBEDDING_DEPLOYMENT_ID\n",
    "LLM_DEPLOYMENT_ID = variables.LLM_DEPLOYMENT_ID\n",
    "EMBEDDING_TABLE = variables.EMBEDDING_TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are again connecting to our SAP HANA Cloud Vector Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('.user.ini')\n",
    "connection = dbapi.connect(\n",
    "    address=config.get('hana', 'url'), \n",
    "    port=config.get('hana', 'port'), \n",
    "    user=config.get('hana', 'user'),\n",
    "    password=config.get('hana', 'passwd'),\n",
    "    autocommit=True,\n",
    "    sslValidateCertificate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for custom documents\n",
    "embeddings = OpenAIEmbeddings(deployment_id=EMBEDDING_DEPLOYMENT_ID)\n",
    "db = HanaDB(\n",
    "    embedding=embeddings, connection=connection, table_name=EMBEDDING_TABLE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are defining which LLM to use during the retrieving process. We then also assign which database to retrieve information from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which model to use\n",
    "chat_llm = ChatOpenAI(deployment_id=LLM_DEPLOYMENT_ID)\n",
    "\n",
    "# Create a retriever instance of the vector store\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of sending the query directly to the LLM, we create a **RetrievalQA** instance and handover the LLM and the database that should be used during the retrieval process. Now we can send our query to the **Retriever**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the QA instance to query llm based on custom documents\n",
    "qa = RetrievalQA.from_llm(llm=chat_llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "# Send query\n",
    "query = \"What is the machine learning model behind the regression model template of Data Attribute Recommendation?\"\n",
    "# query = \"What is the premium edition of Document Information Extraction?\"\n",
    "# query = \"What does Blocks of 100 Documents for Premium Edition mean?\"\n",
    "\n",
    "print(qa.invoke(query))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
