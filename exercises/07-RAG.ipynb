{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the Retrieval for a Retrieval Augmented Generation (RAG) Use Case\n",
    "Now that we have all our context information stored in SAP HANA Cloud Vector Store, we can start asking the LLM questions about SAP AI Services. This time the model will not respond from it's knowledge base, that is what it knows from it's training data but the retriever will check for relevant context information in our vector database and send that text chunk to the LLM to read through before responding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain_community.vectorstores.hanavector import HanaDB\n",
    "from hdbcli import dbapi\n",
    "\n",
    "import configparser\n",
    "import variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‰ SET the `EMBEDDING_TABLE` to `\"EMBEDDINGS_SAP_AI_SERVICES_>add your name here<\"` like in the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are again connecting to our SAP HANA Cloud Vector Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('.user.ini')\n",
    "connection = dbapi.connect(\n",
    "    address=config.get('hana', 'url'), \n",
    "    port=config.get('hana', 'port'), \n",
    "    user=config.get('hana', 'user'),\n",
    "    password=config.get('hana', 'passwd'),\n",
    "    autocommit=True,\n",
    "    sslValidateCertificate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for custom documents\n",
    "embeddings = OpenAIEmbeddings(deployment_id=variables.EMBEDDING_DEPLOYMENT_ID)\n",
    "db = HanaDB(\n",
    "    embedding=embeddings, connection=connection, table_name=variables.EMBEDDING_TABLE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are defining which LLM to use during the retrieving process. We then also assign which database to retrieve information from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which model to use\n",
    "chat_llm = ChatOpenAI(deployment_id=variables.LLM_DEPLOYMENT_ID)\n",
    "\n",
    "# Create a retriever instance of the vector store\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Now instead of sending the query directly to the LLM, you will create a `RetrievalQA` instance and handover the LLM and the database that should be used during the retrieval process. Now you can send your query to the `Retriever`.\n",
    "\n",
    "ğŸ‘‰ Try out different queries. You can ask anything you would like to know about the SAP AI Services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the QA instance to query llm based on custom documents\n",
    "qa = RetrievalQA.from_llm(llm=chat_llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "# Send query\n",
    "query = \"What is the machine learning model behind the regression model template of Data Attribute Recommendation?\"\n",
    "# query = \"What is the premium edition of Document Information Extraction?\"\n",
    "# query = \"What does Blocks of 100 Documents for Premium Edition mean?\"\n",
    "\n",
    "print(qa.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Go back to [06-store-embeddings-hana](06-store-embeddings-hana.ipynb) and try out different chunk sizes or different values for overlap. Store these chunks in a different table by adding a new variable to [variables.py](variables.py) and run this script again using the newly created table.\n",
    "\n",
    "[Next exercise](08-semantic-chunking.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
