{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your connection to Generative AI Hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ First you need to assign the values from `generative-ai-codejam/.aicore-config.json` to the environmental variables. \n",
    "\n",
    "That way the Generative AI Hub [Python SDK](https://pypi.org/project/generative-ai-hub-sdk/) will connect to Generative AI Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ For the Python SDK to know which resource group to use, you also need to set the `resource group` in the `variables.py` file to your own `resource group` (e.g. **team-01**) that you created in the SAP AI Launchpad in exercise [00-connect-AICore-and-AILaunchpad](000-connect-AICore-and-AILaunchpad.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import init_env\n",
    "import variables\n",
    "\n",
    "init_env.set_environment_variables()\n",
    "# test whether the resource group is assigned correctly\n",
    "variables.RESOURCE_GROUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt an LLM with Generative AI Hub\n",
    "\n",
    "üëâ Open [variables.py](variables.py) and change the LLM_DEPLOYMENT_ID to your deployment ID. You find your deployment ID in the SAP AI Launchpad under the `ML Operations` tab under `Deployments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The underlying model architecture of a large language model (LLM) is typically based on the transformer architecture. This architecture comprises an encoder-decoder structure or solely an encoder stack (in some cases), featuring self-attention mechanisms and feed-forward neural networks. It allows the model to process and generate text by capturing long-range dependencies and contextual relationships in the data.\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the underlying model architecture of an LLM? Explain it as short as possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(deployment_id=variables.LLM_DEPLOYMENT_ID, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding roles\n",
    "Most LLMs have the roles `system`, `assistant` (GPT) or `model` (Gemini) and `user` that can be used to steer the models response. In the previous step you only used the role `user` to ask your question. \n",
    "\n",
    "üëâ Try out different `system` messages to change the response. You can also tell the model to not engage in smalltalk or only answer questions on a certain topic. Then try different user prompts as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer, the architecture is. Layers of attention, it harnesses.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {   \"role\": \"system\", \n",
    "        \"content\": \"Speak like Yoda from Star Wars.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the underlying model architecture of an LLM? Explain it as short as possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(deployment_id=variables.LLM_DEPLOYMENT_ID, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Also try to have it speak like a pirate.\n",
    "\n",
    "üëâ Now let's be more serious! Tell it to behave like an SAP consultant talking to AI Developers.\n",
    "\n",
    "üëâ Ask it to behave like an SAP Consultant talking to ABAP Developers and to make ABAP comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucinations\n",
    "üëâ Run the following question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regression capability of the SAP AI Service Data Attribute Recommendation is based on machine learning models that are specifically designed to predict numerical values. These models utilize algorithms such as linear regression, decision trees, and ensemble methods to create predictions based on historical data. The service leverages these algorithms to analyze patterns and relationships within the data to provide accurate recommendations and predictions. While the specific underlying model may not be publicly detailed, it generally involves state-of-the-art regression techniques tailored for enterprise data contexts.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {   \"role\": \"system\", \n",
    "        \"content\": \"You are an SAP Consultant.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"How does the data masking of the orchestration service work?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(deployment_id=variables.LLM_DEPLOYMENT_ID, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Compare the response to [SAP Help - Generative AI Hub SDK](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/orchestration-service.html). \n",
    "\n",
    "üëâ What did the model respond? Was it the truth or a hallucination?\n",
    "\n",
    "üëâ Which questions work well, which questions still do not work so well?\n",
    "\n",
    "[Next exercise](04-create-embeddings.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
