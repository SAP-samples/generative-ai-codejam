{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your connection to Generative AI Hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ First you need to assign the values from `generative-ai-codejam/.aicore-config.json` to the environmental variables. \n",
    "\n",
    "That way the Generative AI Hub [Python SDK](https://pypi.org/project/generative-ai-hub-sdk/) will connect to Generative AI Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ For the Python SDK to know which resource group to use, you also need to set the `resource group` in the `variables.py` file to your own `resource group` (e.g. **team-01**) that you created in the SAP AI Launchpad in exercise [00-connect-AICore-and-AILaunchpad](000-connect-AICore-and-AILaunchpad.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init_env\n",
    "import variables\n",
    "\n",
    "# TODO: You need to specify which model you want to use. In this case we are directing our prompt\n",
    "# to the openAI API directly so you need to pick one of the GPT models. Make sure the model is actually deployed\n",
    "# in genAI Hub. You might also want to chose a model that can also process images here already. E.g. 'gpt-4o-mini'\n",
    "MODEL_NAME = ''\n",
    "\n",
    "init_env.set_environment_variables()\n",
    "# test whether the resource group is assigned correctly\n",
    "variables.RESOURCE_GROUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt an LLM with Generative AI Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the underlying model architecture of an LLM? Explain it as short as possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "\n",
    "response = chat.completions.create(**kwargs)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding roles\n",
    "Most LLMs have the roles `system`, `assistant` (GPT) or `model` (Gemini) and `user` that can be used to steer the models response. In the previous step you only used the role `user` to ask your question. \n",
    "\n",
    "üëâ Try out different `system` messages to change the response. You can also tell the model to not engage in smalltalk or only answer questions on a certain topic. Then try different user prompts as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {   \"role\": \"system\", \n",
    "        # TODO try changing the system prompt\n",
    "        \"content\": \"Speak like Yoda from Star Wars.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the underlying model architecture of an LLM? Explain it as short as possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Also try to have it speak like a pirate.\n",
    "\n",
    "üëâ Now let's be more serious! Tell it to behave like an SAP consultant talking to AI Developers.\n",
    "\n",
    "üëâ Ask it to behave like an SAP Consultant talking to ABAP Developers and to make ABAP comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucinations\n",
    "üëâ Run the following question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {   \"role\": \"system\", \n",
    "        \"content\": \"You are an SAP Consultant.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"How does the data masking of the orchestration service work?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Compare the response to [SAP Help - Generative AI Hub SDK](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/orchestration-service.html). \n",
    "\n",
    "üëâ What did the model respond? Was it the truth or a hallucination?\n",
    "\n",
    "üëâ Which questions work well, which questions still do not work so well?\n",
    "\n",
    "# Use Multimodal Models\n",
    "\n",
    "Multimodal models can use different inputs such as text, audio and images. In Generative AI Hub on SAP AI Core you can access multiple multimodal models (e.g. gpt-4o-mini).\n",
    "\n",
    "üëâ If you have not deployed one of the gpt-4o-mini models in previous exercises, then go back to the model library and deploy a model that can also process images.\n",
    "\n",
    "üëâ Now run the code snippet below to get a description for the AI Foundation Architecture. These descriptions can then for example be used as alternative text for screen readers or other assistive tech.\n",
    "\n",
    "üëâ Download your own image and play around with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# get the image from the documents folder\n",
    "with open(\"documents/ai-foundation-architecture.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe the images as an alternative text.\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "            }]\n",
    "        }]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting text from images\n",
    "I love bananabread and I think recipes are a good example of how LLMs can also extract complex text from images. Try your own recipe if you like :)\n",
    "This exercise also shows how you can use the output of an LLM in other systems, as you can tell the LLM how to output information, for example in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the image from the documents folder\n",
    "with open(\"documents/bananabread.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Extract the ingredients and instructions in two different json files.\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "            }]\n",
    "        }]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Next exercise](04-create-embeddings.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
