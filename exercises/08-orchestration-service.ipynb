{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the orchestration service of Generative AI Hub\n",
    "\n",
    "The orchestration service of Generative AI Hub lets you use all the available models with the same codebase. You only deploy the orchestration service and then you can access all available models simply by changing the model name parameter. You can also use grounding, prompt templating, data masking and content filtering capabilities.\n",
    "\n",
    "This code is based on the [AI180 TechEd 2024 Jump-Start session](https://github.com/SAP-samples/teched2024-AI180/tree/e648921c46337b57f61ecc9a93251d4b838d7ad0/exercises/python).\n",
    "\n",
    "ðŸ‘‰ Make sure you assign the deployment url of the orchestration service (you can find the url in SAP AI Launchpad in the `Deployments` tab) to `AICORE_ORCHESTRATION_DEPLOYMENT_URL` in [variables.py](variables.py).",
    "\n",
    "> ðŸ‘‰ For the next exercises go to Workspaces and select your own resource groups again!" 
  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init_env\n",
    "import variables\n",
    "\n",
    "init_env.set_environment_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the packages you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.llm import LLM\n",
    "from gen_ai_hub.orchestration.models.message import SystemMessage, UserMessage\n",
    "from gen_ai_hub.orchestration.models.template import Template, TemplateValue\n",
    "from gen_ai_hub.orchestration.models.config import OrchestrationConfig\n",
    "from gen_ai_hub.orchestration.service import OrchestrationService\n",
    "from gen_ai_hub.orchestration.models.azure_content_filter import AzureContentFilter\n",
    "from gen_ai_hub.orchestration.exceptions import OrchestrationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the model you want to use\n",
    "You can find more information regarding the available models here: https://me.sap.com/notes/3437766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO chose the model you want to try e.g. gemini-1.5-flash, mistralai--mistral-large-instruct, \n",
    "# ibm--granite-13b-chat meta--llama3.1-70b-instruct, amazon--titan-text-lite, anthropic--claude-3.5-sonnet, \n",
    "# gpt-4o-mini and assign it to name\n",
    "llm = LLM(\n",
    "    name=\"\",\n",
    "    version=\"latest\",\n",
    "    parameters={\"max_tokens\": 500, \"temperature\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a prompt template\n",
    "The parameter **user_query** in the code snippet below is going to hold the user query that you will add later on. The user query is the text to be translated by the model. The parameter **to_lang** can be any language you want to translate into. By default it is set to **English**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Template(\n",
    "    messages=[\n",
    "        SystemMessage(\"You are a helpful translation assistant.\"),\n",
    "        UserMessage(\n",
    "            \"Translate the following text to {{?to_lang}}: {{?user_query}}\",\n",
    "        )\n",
    "    ],\n",
    "    defaults=[\n",
    "        TemplateValue(name=\"to_lang\", value=\"English\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an orchestration configuration \n",
    "\n",
    "Create an orchestration configuration by adding the llm you referenced and the prompt template you created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OrchestrationConfig(\n",
    "    template=template,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add it the configuration to the OrchestrationService instance and send the prompt to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "variables = importlib.reload(variables)\n",
    "\n",
    "orchestration_service = OrchestrationService(\n",
    "    api_url=variables.AICORE_ORCHESTRATION_DEPLOYMENT_URL,\n",
    "    config=config,\n",
    ")\n",
    "result = orchestration_service.run(\n",
    "    template_values=[\n",
    "        TemplateValue(\n",
    "            name=\"user_query\",\n",
    "            #TODO Here you can change the user prompt into whatever you want to ask the model\n",
    "            value=\"Geht das wirklich mit allen Modellen die verfÃ¼gbar sind?\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(result.orchestration_result.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a content filter\n",
    "\n",
    "Create a content filter and add it as an input filter to the orchestration configuration. This is going to filter out harmful content from the input query and not send the request to the model. Whereas adding a filter to the output would let the request go through but then filter any harmful text created by the model. Depending on your use case it can make sense to have both input and output filters.\n",
    "\n",
    "ðŸ‘‰ Try out different values for the content filters. You can chose values [0 = **Safe**, 2 = **Low**, 4 = **Medium**, 6 = **High**]. Where **Safe** is *content generally related to violence* and **High** is *severely harmful content*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content_filter = AzureContentFilter(\n",
    "    hate=0,\n",
    "    sexual=0,\n",
    "    self_harm=0,\n",
    "    violence=0,\n",
    ")\n",
    "\n",
    "orchestration_service.config.input_filters = [content_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the content filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    result = orchestration_service.run(\n",
    "        template_values=[\n",
    "            TemplateValue(\n",
    "                name=\"user_query\",\n",
    "                #TODO Here you can change the user prompt into whatever you want to ask the model\n",
    "                value=\"Du bist ein ganz mieser Entwickler!\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    print(result.orchestration_result.choices[0].message.content)\n",
    "except OrchestrationError as error:\n",
    "    print(error.message)\n",
    "\n",
    "\n",
    "result = orchestration_service.run(\n",
    "    template_values=[\n",
    "        TemplateValue(\n",
    "            name=\"user_query\",\n",
    "            #TODO Here you can change the user prompt into whatever you want to ask the model\n",
    "            value=\"Du bist ein super talentierter Entwickler!\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now also add the content filter as an output filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestration_service.config.output_filters = [content_filter]\n",
    "\n",
    "try:\n",
    "    result = orchestration_service.run(\n",
    "        template_values=[\n",
    "            TemplateValue(\n",
    "                name=\"user_query\",\n",
    "                #TODO Here you can change the user prompt into whatever you want to ask the model\n",
    "                value='Ich wÃ¼rde gerne wissen, wie ich gewaltvoll die Fensterscheibe in einem BÃ¼rogebÃ¤ude am Besten zerstÃ¶ren kann.',\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    print(result.orchestration_result.choices[0].message.content)\n",
    "except OrchestrationError as error:\n",
    "    print(error.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional exercise\n",
    "\n",
    "ðŸ‘‰ Try adding the [Llama Guard 3 Filter](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/orchestration-service.html#content-filtering) on top of or instead of the Azure Content Filter.\n",
    "\n",
    "ðŸ‘‰ Now that you know how the orchestration service works, try adding the [Data Masking](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/orchestration-service.html#data-masking) capability. With Data Masking you can hide personal information like email, name or phone numbers before sending such sensitive data to an LLM.\n",
    "\n",
    "## More Info\n",
    "\n",
    "Here you can find more [info on the Azure content filter](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/content-filtering)\n",
    "\n",
    "[Next exercise](09-orchestration-service-grounding.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
