{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agents with Generative AI Hub\n",
    "\n",
    "Everyone is talking about AI Agents and how they are going to make LLMs more powerful. AI Agents are basically systems that have a set of tools that they can use and the LLM is the agents brain to decide which tool to use when. Some systems are more agentic than others, depending on the amount of autonomy and decision-making power they possess. \n",
    "\n",
    "If you want to know more about AI Agents and how to build one from scratch, check out this [Devtoberfest session](https://community.sap.com/t5/devtoberfest/getting-started-with-agents-using-sap-generative-ai-hub/ev-p/13865119).\n",
    "\n",
    "ðŸ‘‰ Before you start you will need to add the constant `LLM_DEPLOYMENT_ID` to [variables.py](variables.py). You can use the `gpt-4.1-mini` model again that we deployed earlier. You find the deployment ID in SAP AI Launchpad."

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\".*tracemalloc.*\")\n",
    "\n",
    "import init_env\n",
    "import variables\n",
    "\n",
    "init_env.set_environment_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the packages you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "from langchain import hub\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will give the agent different tools to use. The first tool will be [access to Wikipedia](https://python.langchain.com/docs/integrations/tools/wikipedia/). This way instead of answering from it's training data, the model will check on Wikipedia articles first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "# Let's check if it works\n",
    "print(wikipedia.run(\"Tbilisi\"))\n",
    "\n",
    "# Assign wikipedia to the set of tools\n",
    "tools = [wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For agents the initial prompt including all the instructions is very important. \n",
    "# LangChain already has a good set of prompts that we can reuse here.\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "variables = importlib.reload(variables)\n",
    "\n",
    "# Create a chat model instance which will act as the brain of the agent\n",
    "llm = ChatOpenAI(deployment_id=variables.LLM_DEPLOYMENT_ID)\n",
    "\n",
    "# Create an agent with the llm, tools and prompt\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "# Let's ask try the agent and ask about the city\n",
    "result = agent.invoke({\n",
    "            \"messages\": [\n",
    "                (\"user\", f\"{prompt}\\n\\nTell me about Tbilisi!\")\n",
    "            ]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the result of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the results\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Display as formatted markdown\n",
    "final_response = result['messages'][-1].content\n",
    "display(Markdown(f\"## Agent Response\\n\\n{final_response}\"))\n",
    "\n",
    "# Or show the reasoning process\n",
    "print(\"=== Agent Thinking Process ===\")\n",
    "for msg in result['messages']:\n",
    "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "        print(f\"ðŸ”§ Agent decided to use tool: {msg.tool_calls[0]['name']}\")\n",
    "    elif msg.__class__.__name__ == 'ToolMessage':\n",
    "        print(f\"ðŸ“‹ Tool returned data (length: {len(msg.content)} chars)\")\n",
    "    elif msg.__class__.__name__ == 'AIMessage' and msg.content:\n",
    "        print(f\"ðŸ¤– Final response ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give the agent access to our HANA vector store\n",
    "Now the agent can use Wikipedia but wouldn't it be nice if the agent could pick between resources and decide when to use what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain & HANA Vector Engine - legacy integration: https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.hanavector.HanaDB.html\n",
    "#from langchain_community.vectorstores.hanavector import HanaDB\n",
    "\n",
    "# SAP HANA integration with LangChain: https://pypi.org/project/langchain-hana \n",
    "from langchain_hana import HanaDB\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# connect to HANA instance\n",
    "connection = init_env.connect_to_hana_db()\n",
    "connection.isconnected()\n",
    "\n",
    "# Reference which embedding model, DB connection and table to use\n",
    "embeddings = OpenAIEmbeddings(deployment_id=variables.EMBEDDING_DEPLOYMENT_ID)\n",
    "db = HanaDB(\n",
    "    embedding=embeddings, connection=connection, table_name=variables.EMBEDDING_TABLE\n",
    ")\n",
    "\n",
    "# Create a retriever instance of the vector store\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# We need to add a description to the retriever so that the llm knows when to use this tool.\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"SAP-orchestration-service-docu\",\n",
    "    \"Search for information SAP's orchestration service. For all inquiries specifically related to the Generative AI Hub and SAP's orchestration service, this tool must be used without exception!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask the Agent\n",
    "\n",
    "You can ask the agent anything in general and it will try to find an entry from Wikipedia, unless you ask about SAP's orchestration service. Then it will get the information from SAP HANA vector store, where it holds the documentation, instead of Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's add the retriever as a tool\n",
    "tools = [wikipedia, retriever_tool]\n",
    "\n",
    "# And create the agent again with the two tools, wikipedia and the HANA vector store (retriever)\n",
    "# Create an agent with the llm, tools and prompt\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "# First ask: What is Data Masking?\n",
    "# Then ask: What is Data Masking in SAP's Generative AI Hub?\n",
    "# Pay attention to the response! Do you see what is happening now?\n",
    "result = agent.invoke({\n",
    "            \"messages\": [\n",
    "                (\"user\", f\"{prompt}\\n\\nWhat is Data Masking in SAP's Generative AI Hub?\")\n",
    "            ]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display as formatted markdown\n",
    "final_response = result['messages'][-1].content\n",
    "display(Markdown(f\"## Agent Response\\n\\n{final_response}\"))\n",
    "\n",
    "# Or show the reasoning process\n",
    "print(\"=== Agent Thinking Process ===\")\n",
    "for msg in result['messages']:\n",
    "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "        print(f\"ðŸ”§ Agent decided to use tool: {msg.tool_calls[0]['name']}\")\n",
    "    elif msg.__class__.__name__ == 'ToolMessage':\n",
    "        print(f\"ðŸ“‹ Tool returned data (length: {len(msg.content)} chars)\")\n",
    "    elif msg.__class__.__name__ == 'AIMessage' and msg.content:\n",
    "        print(f\"ðŸ¤– Final response ready\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
