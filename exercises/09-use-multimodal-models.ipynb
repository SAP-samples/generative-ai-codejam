{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Multimodal Models\n",
    "\n",
    "This exercise is optional if you have more time and would like to try out multimodal models. Multimodal models can use different inputs such as text, audio and images. In Generative AI Hub on SAP AI Core you can access `gpt-4o` which is multimodal.\n",
    "\n",
    "ðŸ‘‰ Go back to [01-deploy-model](01-deploy-model.md) to deploy gpt-4o.\n",
    "\n",
    "ðŸ‘‰ Then assign the deployment id to the `DEPLOYMENT_ID_MULTIMODAL` in [variables.py](variables.py).\n",
    "\n",
    "ðŸ‘‰ Now run the code snippet below to get a description for the AI Foundation Architecture. These descriptions can then for example be used as alternative text for screen readers or other assistive tech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "import init_env\n",
    "import variables\n",
    "\n",
    "init_env.set_environment_variables()\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents/ai-foundation-architecture.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "message= {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe the images as an alternative text\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "            }\n",
    "        ]}\n",
    "    \n",
    "\n",
    "model = ChatOpenAI(deployment_id=variables.DEPLOYMENT_ID_MULTIMODAL)\n",
    "\n",
    "response = model.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents/bananabread.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "message= {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Extract the ingredients and instructions in two different json files\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "            }\n",
    "        ]}\n",
    "    \n",
    "\n",
    "model = ChatOpenAI(deployment_id=variables.DEPLOYMENT_ID_MULTIMODAL)\n",
    "\n",
    "response = model.invoke([message])\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
