{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic chunker\n",
    "Now do the same thing using the `Semantic Chunker`. You will store the embeddings in a different table by adding `+\"_SEMANTIC\"` to the table name. LangChains implementation of the `Semantic Chunker` is based on [Greg Kamradt's work](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb), there you can also find more information on the `Semantic Chunker` and other chunking techniques.\n",
    "\n",
    "ðŸ‘‰ For the semantic chunker we need to install a new package: [langchain-experimental](https://pypi.org/project/langchain-experimental/). Run the following command in your terminal. Make sure to run it inside the virtual environment.\n",
    "\n",
    "```sh\n",
    "pip install langchain-experimental\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "from langchain_community.vectorstores.hanavector import HanaDB\n",
    "\n",
    "from hdbcli import dbapi\n",
    "import configparser\n",
    "\n",
    "import variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to HANA\n",
    "config = configparser.ConfigParser()\n",
    "config.read('.user.ini')\n",
    "connection = dbapi.connect(\n",
    "    address=config.get('hana', 'url'), \n",
    "    port=config.get('hana', 'port'), \n",
    "    user=config.get('hana', 'user'),\n",
    "    password=config.get('hana', 'passwd'),\n",
    "    autocommit=True,\n",
    "    sslValidateCertificate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom documents\n",
    "loader = PyPDFDirectoryLoader('documents/')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding instance is used during semantic chunking\n",
    "embeddings = OpenAIEmbeddings(deployment_id=variables.EMBEDDING_DEPLOYMENT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create semantic text chunks\n",
    "text_splitter = SemanticChunker(embeddings)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"Number of document chunks: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db = HanaDB(\n",
    "    embedding=embeddings, connection=connection, table_name=variables.SEMANTIC_EMBEDDING_TABLE\n",
    ")\n",
    "\n",
    "# Delete already existing documents from the table\n",
    "db.delete(filter={})\n",
    "\n",
    "# add the loaded document chunks\n",
    "db.add_documents(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the embeddings in SAP HANA Cloud Vector Engine\n",
    "\n",
    "ðŸ‘‰ Go back to [HANA Cloud Database Explorer](https://central-hana-cloud-instance-us-77belmsm.hana-tooling.ingress.orchestration.prod-us10.hanacloud.ondemand.com/hrtt/sap/hana/cst/catalog/cockpit-index.html?target=sqlconsole&databaseid=C2366).\n",
    "\n",
    "ðŸ‘‰ Open a second tab and compare the embedding table with the semantic chunks to the chunks using the standard character text splitter:\n",
    "\n",
    "```sql\n",
    "SELECT VEC_TEXT, VEC_META, TO_NVARCHAR(VEC_VECTOR) FROM EMBEDDINGS_CODEJAM_+<YOUR_NAME>+\"_SEMANTIC\"\n",
    "```\n",
    "\n",
    "ðŸ‘‰ Go back to exercise [07-RAG](07-RAG.ipynb) and run the RAG code again but this time use the table with the newly created semantic chunks.\n",
    "\n",
    "[Next exercise](09-use-multimodal-models.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
